{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Airbnb Listings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Problem Statement\n",
    "\n",
    "The aim of this project\n",
    "\n",
    "- different strategies:\n",
    "- different models:\n",
    "- classification algorithms:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Configuration\n",
    "We import all the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-19T01:43:27.646157Z",
     "start_time": "2023-06-19T01:43:03.722150Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspellchecker in /opt/conda/lib/python3.11/site-packages (0.7.2)\r\n",
      "Collecting vaderSentiment\r\n",
      "  Using cached vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from vaderSentiment) (2.31.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->vaderSentiment) (3.1.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->vaderSentiment) (2.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->vaderSentiment) (1.26.16)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->vaderSentiment) (2023.5.7)\r\n",
      "Installing collected packages: vaderSentiment\r\n",
      "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 13] Permission denied: '/opt/conda/lib/python3.11/site-packages/vaderSentiment/__init__.py'\r\n",
      "Consider using the `--user` option or check the permissions.\r\n",
      "\u001b[0m\u001b[31m\r\n",
      "\u001b[0mRequirement already satisfied: contractions in /opt/conda/lib/python3.11/site-packages (0.1.73)\r\n",
      "Requirement already satisfied: textsearch>=0.0.21 in /opt/conda/lib/python3.11/site-packages (from contractions) (0.0.24)\r\n",
      "Requirement already satisfied: anyascii in /opt/conda/lib/python3.11/site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\r\n",
      "Requirement already satisfied: pyahocorasick in /opt/conda/lib/python3.11/site-packages (from textsearch>=0.0.21->contractions) (2.0.0)\r\n",
      "Requirement already satisfied: gensim in /opt/conda/lib/python3.11/site-packages (4.3.1)\r\n",
      "Requirement already satisfied: numpy>=1.18.5 in /opt/conda/lib/python3.11/site-packages (from gensim) (1.24.3)\r\n",
      "Requirement already satisfied: scipy>=1.7.0 in /opt/conda/lib/python3.11/site-packages (from gensim) (1.10.1)\r\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.11/site-packages (from gensim) (6.3.0)\r\n",
      "Requirement already satisfied: deep-translator in /opt/conda/lib/python3.11/site-packages (1.11.1)\r\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /opt/conda/lib/python3.11/site-packages (from deep-translator) (4.12.2)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /opt/conda/lib/python3.11/site-packages (from deep-translator) (2.31.0)\r\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.11/site-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.3.2.post1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.1.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (1.26.16)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2023.5.7)\r\n",
      "\u001b[31mERROR: Invalid requirement: '=='\u001b[0m\u001b[31m\r\n",
      "\u001b[0mRequirement already satisfied: transformers in /opt/conda/lib/python3.11/site-packages (4.30.2)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (2.31.0)\r\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.11/site-packages (4.12.2)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers) (3.12.2)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.15.1)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (1.24.3)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (23.1)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (6.0)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (2023.6.3)\r\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.13.3)\r\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.3.1)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers) (4.56.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests) (3.1.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests) (2.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests) (1.26.16)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests) (2023.5.7)\r\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.11/site-packages (from beautifulsoup4) (2.3.2.post1)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspellchecker\n",
    "!pip install vaderSentiment\n",
    "!pip install contractions\n",
    "!pip install gensim\n",
    "!pip install -U deep-translator\n",
    "\n",
    "!pip install torch == 2.0.0+cu118 torchvision == 0.15.1+cu118 torchaudio == 2.0.1 --index-url https: // download.pytorch.org/whl/cu118\n",
    "!pip install transformers requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-19T01:51:32.755674Z",
     "start_time": "2023-06-19T01:51:32.722227Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Utility\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "from matplotlib import style\n",
    "\n",
    "style.use('ggplot')\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Extra tools for data preprocessing\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from collections import OrderedDict\n",
    "from langdetect import detect\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "# Vader\n",
    "import vaderSentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Downloader for Glove Word Embedding\n",
    "import gensim\n",
    "import gensim.downloader as gloader\n",
    "\n",
    "# WordCloud\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "# Contractions\n",
    "import contractions\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Pytorch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "#####################\n",
    "#  REPRODUCIBILITY  #\n",
    "#####################\n",
    "\n",
    "# Seed value\n",
    "SEED_VALUE = 42\n",
    "\n",
    "# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED_VALUE)\n",
    "\n",
    "# 2. Set `python` built-in pseudo-random generator at a fixed value\n",
    "random.seed(SEED_VALUE)\n",
    "\n",
    "# 3. Set `numpy` pseudo-random generator at a fixed value\n",
    "np.random.seed(SEED_VALUE)\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"bs4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-19T01:51:54.607185Z",
     "start_time": "2023-06-19T01:51:35.444916Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bert_base_multilingual_tokenizer = AutoTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
    "bert_base_multilingual_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'nlptown/bert-base-multilingual-uncased-sentiment')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## The Airbnb Unlisting Dataset\n",
    "The dataset provided for this project is a subset of the Airbnb Unlisting Dataset. It contains information about Airbnb properties in Lisbon, Portugal, and their status (listed or unlisted) in the first quarter of 2018.\n",
    "The dataset does not provide a sentiment label assigned to each review (*positive* or *negative*); to address this problem we can use the following strategies:\n",
    "- use **VADER** (Valence Aware Dictionary and sEntiment Reasoner)\n",
    "- use **BERT** (Bidirectional Encoder Representations from Transformers)\n",
    "\n",
    "But first let's take a look at the data.\n",
    "\n",
    "### Dataset structure\n",
    "\n",
    "__The data is divided in following sets:__\n",
    "\n",
    "* __Train (train.xlsx) (12,496 lines):__\n",
    "\n",
    "Contains the Airbnb and host descriptions (“description” and “host_about” columns), as well as the information regarding the property listing status (“unlisted” column). A property is considered unlisted (1) if it got removed from the quarterly Airbnb list and it is considered listed (1) if it remains on that same list.\n",
    "- **index** (numerical): unique identifier associated to the Airbnb property\n",
    "- **description** (text): Airbnb description\n",
    "- **host_about** (text): host description\n",
    "- **unlisted** (binary): 1 if the property is unlisted, 0 otherwise\n",
    "\n",
    "**ex:** {\"**index**\": \"1\", \"**description**\": \"This is a shared mixed room in our hostel, with shared bathroom.`<br />`We are located right across the street from subway station Parque, we are 5 min walk to Marques de Pombal square.`<br /><br />`...\", \"**host_about**\": \"Alojamento Local Registro: 20835/AL\", \"**unlisted**\": \"0\"}\n",
    "\n",
    "--------\n",
    "* __Train Reviews (train_reviews.xlsx) (72,1402):__\n",
    "\n",
    "This file has all the guests’ comments made to each Airbnb property. Note that there can be more than one comment per property, not all properties have comments, and comments can appear in many languages!\n",
    "- **index** (numerical): unique identifier associated to the Airbnb property\n",
    "- **comments** (text): guest comment\n",
    "\n",
    "**ex:** {\"**index**\": \"1\", \"**comments**\": \"The host canceled this reservation 2 days before arrival. This is an automated posting.\"}\n",
    "\n",
    "--------\n",
    "* __Test (test.xlsx) (1,389 lines):__\n",
    "\n",
    "The structure of this dataset is the same as the train set, except that it does not contain the “unlisted” column. The teaching team is keeping this information secret! You are expected to provide the predicted status (0 or 1) for each Airbnb in this set. Once the projects are delivered, we will compare your predictions with the actual (true) labels.\n",
    "\n",
    "* __Test Reviews (test_reviews.xlsx) (80,877):__\n",
    "\n",
    "The structure of this dataset is the same as the train reviews set, but the comments correspond to the properties present on the test set"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Data Import\n",
    "\n",
    "It starts with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-19T01:52:06.198840Z",
     "start_time": "2023-06-19T01:51:54.660117Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "current_directory = os.getcwd()\n",
    "\n",
    "corpora_train = pd.read_excel(os.path.join(f\"{current_directory}/corpora\", 'train.xlsx'))\n",
    "corpora_train_review = pd.read_excel(os.path.join(f\"{current_directory}/corpora\", 'test_reviews.xlsx'))\n",
    "corpora_test = pd.read_excel(os.path.join(f\"{current_directory}/corpora\", 'test.xlsx'))\n",
    "corpora_test_review = pd.read_excel(os.path.join(f\"{current_directory}/corpora\", 'test_reviews.xlsx'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Data Inspection\n",
    "\n",
    "In this section we are going to briefly inspect the data.\n",
    "\n",
    "**Done in the other notebook**\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Data Understanding\n",
    "\n",
    "Before working on our dataset, we want to explore it to find out interesting insights and irregularities.\n",
    "\n",
    "We will look at some features and try to find out interesting facts and patterns from them.\n",
    "**This Section done in the other notebook|**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Text Preprocessing\n",
    "\n",
    "During the **Data Understanding** phase we have already removed some noisy data. Let's continue the exploration of data and perform some preprocessing."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Review Preprocessing\n",
    "In this section we are going to preprocess the **reviews**.\n",
    "\n",
    "We had to define three types of preprocessing.\n",
    "\n",
    "In particular:\n",
    "* The first preprocessing pipeline will be used to extract the `clean_text` to be used later.\n",
    "* The second preprocessing pipeline will be used to extract the `clean_text` to be exploited in **VADER-labeling**.\n",
    "* The second preprocessing pipeline will be used to extract the `clean_text` to be exploited in **BERT-labeling**.\n",
    "* The third preprocessing pipeline will be used to extract the `clean_text` to be exploited in the construction of the **Dense Word Embedding**.\n",
    "\n",
    "The order of execution within the pipelines is important.\n",
    "\n",
    "\n",
    "**Pipeline 1**:\n",
    "* strip html\n",
    "* strip text\n",
    "* remove stopwords\n",
    "* replace special characters\n",
    "* filter out uncommon symbols\n",
    "* combine whitespace\n",
    "* lower text\n",
    "* stemming\n",
    "\n",
    "\n",
    "**Pipeline 2**:\n",
    "* strip html\n",
    "* strip text\n",
    "\n",
    "**Pipeline 3**:\n",
    "* strip html\n",
    "* strip text\n",
    "* remove stopwords\n",
    "* replace special characters\n",
    "* filter out uncommon symbols\n",
    "* combine whitespace\n",
    "* lower text\n",
    "* expand contractions\n",
    "\n",
    "**Pipeline 4**:\n",
    "* strip html\n",
    "* strip text\n",
    "* remove stopwords\n",
    "* replace special characters\n",
    "* filter out uncommon symbols\n",
    "* combine whitespace\n",
    "* lower text\n",
    "* expand contractions\n",
    "\n",
    "---------------------------\n",
    "\n",
    "***NOTES ABOUT PIPELINE 2***:\n",
    "\n",
    "As you can see from the lists above, in the second pipeline (compared to the first):\n",
    "- we do not remove the punctuation and special characters\n",
    "- we do not lower the text\n",
    "- we do not perform stemming\n",
    "- we do not remove the stopwords\n",
    "\n",
    "We need to use this pipeline because Bert assigns a polarity in reference to the following as well:\n",
    "\n",
    "- Punctuation\n",
    "- Capitalization\n",
    "- Conjunctions\n",
    "- Preceding Tri-gram\n",
    "- Emojis, Slangs, and Emoticons\n",
    "---------------------------\n",
    "\n",
    "***NOTES ABOUT PIPELINE 3***:\n",
    "\n",
    "As you can see from the lists above, in the third pipeline (compared to the first):\n",
    "- we do not perform stemming\n",
    "- we expand contractions\n",
    "\n",
    "We need to use this pipeline because otherwise we would have too much OOV terms (**Glove-300**, the pretrained word embedding that we will use, is trained on non-stemmed words).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-19T01:52:06.227560Z",
     "start_time": "2023-06-19T01:52:06.175412Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you very much Antonio ! All has been perfect during our stay, and the appartment is perfectly located in your fabulous city. We would love to visit you again next time :)_x000D_<br/> \n",
      "\n",
      "Very nice appartment in the old town of Lissabon, quite central but still calm in a small lane. No traffic noises etc.! There was enough space for 6 people, everything was clean, kitchen full equipped. Nice contact with the owner. Recommended! \n",
      "\n",
      "When travelling we're looking for kids friendly places to stay, and Antonios place was such a place. It's spacious and well equipped._x000D_<br/>_x000D_<br/>He's friendly mother was at the apartment to greet us and she had made ready a baby bed, a high chair and bought cookies,fruit and buns. Very nice._x000D_<br/>_x000D_<br/>The apartment had a hint of damp smell upon arriving, but after we have had the heaters on for some time it disappeared. So stay in the apartment for more than 15 minuttes._x000D_<br/>_x000D_<br/>The neighborhood is nice and we found good restaurants, shops and playgrounds nearby._x000D_<br/>_x000D_<br/>I'll recommend the apartment  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check first three reviews\n",
    "for i in corpora_train_review['comments'][0:3]:\n",
    "    print(i, '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Reviews Sentiment text classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "There are many words that include not, like needn't. These words are key parts of emotional analysis, so we will remove them from stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# List of preserved negations\n",
    "NEG_LIST = ['nor',\n",
    "            'no',\n",
    "            'needn',\n",
    "            'weren',\n",
    "            'hasn\\'t',\n",
    "            'isn\\'t',\n",
    "            'wasn',\n",
    "            'don\\'t',\n",
    "            'couldn\\'t',\n",
    "            'don',\n",
    "            'hasn',\n",
    "            'won\\'t',\n",
    "            'must',\n",
    "            'didn',\n",
    "            'can\\'t'\n",
    "            'haven\\'t',\n",
    "            'weren\\'t ',\n",
    "            'didn\\'t',\n",
    "            'mustn\\'t',\n",
    "            'wouldn\\'t',\n",
    "            'doesn\\'t',\n",
    "            'needn\\'t',\n",
    "            'wasn\\'t',\n",
    "            'aren\\'t',\n",
    "            'couldn',\n",
    "            'isn',\n",
    "            'dosen',\n",
    "            'shouldn\\'t',\n",
    "            'mightn',\n",
    "            'mightn\\'t',\n",
    "            'not',\n",
    "            'never'\n",
    "            'aren',\n",
    "            \"aren't\",\n",
    "            'couldn',\n",
    "            \"couldn't\",\n",
    "            'didn',\n",
    "            \"didn't\",\n",
    "            'doesn',\n",
    "            \"doesn't\",\n",
    "            'hadn',\n",
    "            \"hadn't\",\n",
    "            'hasn',\n",
    "            \"hasn't\",\n",
    "            'haven',\n",
    "            \"haven't\",\n",
    "            'isn',\n",
    "            \"isn't\",\n",
    "            'ma',\n",
    "            'mightn',\n",
    "            \"mightn't\",\n",
    "            'mustn',\n",
    "            \"mustn't\",\n",
    "            'needn',\n",
    "            \"needn't\",\n",
    "            'shan',\n",
    "            \"shan't\",\n",
    "            'shouldn',\n",
    "            \"shouldn't\",\n",
    "            'wasn',\n",
    "            \"wasn't\",\n",
    "            'weren',\n",
    "            \"weren't\",\n",
    "            'won',\n",
    "            \"won't\",\n",
    "            'wouldn',\n",
    "            \"wouldn't\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Preprocessing Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-19T01:52:48.889117Z",
     "start_time": "2023-06-19T01:52:48.783514Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_reviews_df = corpora_train_review.copy()\n",
    "\n",
    "train_reviews_df['comments'] = train_reviews_df['comments'].astype(str)  # Convert to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-19T01:52:49.449692Z",
     "start_time": "2023-06-19T01:52:49.402654Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;:.#+_?!\"$%&@~]\\t')\n",
    "GOOD_SYMBOLS_RE = re.compile('[^0-9a-zA-Z ]')\n",
    "COMBINE_WHITESPACE = re.compile(r\"\\s+\")\n",
    "\n",
    "translator = GoogleTranslator(source='auto', target='en')\n",
    "\n",
    "class CleanText:\n",
    "\n",
    "    # Translation\n",
    "    def translate(self, text):\n",
    "        \"\"\"\n",
    "        Translates text to english\n",
    "        \"\"\"\n",
    "        translated = translator.translate(text)\n",
    "        return str(translated)\n",
    "\n",
    "    # STOPWORDS\n",
    "    def stopwords(self, text):\n",
    "        \"\"\"Removes english stopwords\"\"\"\n",
    "        stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "        stop_words_no_neg = stop_words - set(NEG_LIST)\n",
    "        text = ' '.join([word for word in text.split() if word not in stop_words_no_neg])\n",
    "        return text\n",
    "\n",
    "    # HTML TAGS\n",
    "    def strip_html(self, text):\n",
    "        \"\"\"\n",
    "        Removes any HTML tags\n",
    "        \"\"\"\n",
    "        soup = BeautifulSoup(text, \"html.parser\")\n",
    "        return soup.get_text()\n",
    "\n",
    "    # STRIP TEXT\n",
    "    def strip_text(self, text):\n",
    "        \"\"\"\n",
    "        Removes any left or right spacing (including carriage return) from text.\n",
    "        Example:\n",
    "        Input: '  This assignment is cool\\n'\n",
    "        Output: 'This assignment is cool'\n",
    "        \"\"\"\n",
    "        return text.strip()\n",
    "\n",
    "    # REPLACE SPECIAL CHARACTER\n",
    "    def replace_special_characters(self, text):\n",
    "        \"\"\"\n",
    "        Replaces special characters, such as paranthesis,\n",
    "        with spacing character\n",
    "        \"\"\"\n",
    "\n",
    "        return REPLACE_BY_SPACE_RE.sub('', text)\n",
    "\n",
    "    # FILTER UNCOMMON SYMBOLS\n",
    "    def filter_out_uncommon_symbols(self, text):\n",
    "        \"\"\"\n",
    "        Removes any special character that is not in the\n",
    "        good symbols list (check regular expression)\n",
    "        \"\"\"\n",
    "        return GOOD_SYMBOLS_RE.sub(' ', text)\n",
    "\n",
    "    # COMBINE WHITESPACE\n",
    "    def combine_whitespace(self, text):\n",
    "        \"\"\"\n",
    "        Removes multiple white-spaces from text.\n",
    "        Example:\n",
    "        Input: 'This    assignment is    cool'\n",
    "        Output: 'This assignment is cool'\n",
    "        \"\"\"\n",
    "        return COMBINE_WHITESPACE.sub(\" \", text).strip()\n",
    "\n",
    "    # LOWER TEXT\n",
    "    def lower(self, text):\n",
    "        \"\"\"\n",
    "        Transforms given text to lower case.\n",
    "        Example:\n",
    "        Input: 'I really like New York city'\n",
    "        Output: 'i really like new york city'\n",
    "        \"\"\"\n",
    "        return text.lower()\n",
    "\n",
    "    # STEMMING\n",
    "    def stemming(self, text):\n",
    "        \"\"\"\n",
    "        Remove the suffixes from the words to\n",
    "        get the root form of the word\n",
    "        Example:\n",
    "        Input: 'Wording'\n",
    "        Output: 'Word'\n",
    "        \"\"\"\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        text = ' '.join(stemmer.stem(token) for token in nltk.word_tokenize(text))\n",
    "        return text\n",
    "\n",
    "    # CONTRACTIONS\n",
    "    def expand_contractions(self, input_text):\n",
    "        \"\"\" Transform contracted words into their standard form. \"\"\"\n",
    "        return contractions.fix(input_text)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    # PIPELINE 1\n",
    "    def transform(self, X, **transform_params):\n",
    "        clean_X = X.apply(self.strip_html) \\\n",
    "            .apply(self.strip_text) \\\n",
    "            .apply(self.stopwords) \\\n",
    "            .apply(self.replace_special_characters) \\\n",
    "            .apply(self.filter_out_uncommon_symbols) \\\n",
    "            .apply(self.combine_whitespace) \\\n",
    "            .apply(self.lower) \\\n",
    "            .apply(self.stemming)\n",
    "\n",
    "        return clean_X\n",
    "\n",
    "    # PIPELINE 2\n",
    "    def transform_vader(self, X, **transform_params):\n",
    "        clean_X = X.apply(self.strip_html) \\\n",
    "            .apply(self.strip_text) \\\n",
    "            #.apply(self.translate)\n",
    "\n",
    "        return clean_X\n",
    "\n",
    "    # PIPELINE 3\n",
    "    def transform_embedding(self, X, **transform_params):\n",
    "        clean_X = X.apply(self.strip_html) \\\n",
    "            .apply(self.strip_text) \\\n",
    "            .apply(self.stopwords) \\\n",
    "            .apply(self.replace_special_characters) \\\n",
    "            .apply(self.filter_out_uncommon_symbols) \\\n",
    "            .apply(self.combine_whitespace) \\\n",
    "            .apply(self.lower) \\\n",
    "            .apply(self.expand_contractions)\n",
    "\n",
    "        return clean_X\n",
    "\n",
    "    # PIPELINE 4\n",
    "    def transform_bert(self, X, **transform_params):\n",
    "        clean_X = X.apply(self.strip_html) \\\n",
    "            .apply(self.strip_text) \\\n",
    "            .apply(self.stopwords) \\\n",
    "            .apply(self.replace_special_characters) \\\n",
    "            .apply(self.filter_out_uncommon_symbols) \\\n",
    "            .apply(self.combine_whitespace) \\\n",
    "            .apply(self.lower) \\\n",
    "            .apply(self.expand_contractions)\n",
    "\n",
    "        return clean_X\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Clean Text Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-19T01:53:22.104616Z",
     "start_time": "2023-06-19T01:52:51.400022Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3505/2952864842.py:30: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30.5 s, sys: 207 ms, total: 30.7 s\n",
      "Wall time: 30.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ct = CleanText()\n",
    "train_reviews_df['clean_text'] = ct.fit(train_reviews_df.comments).transform(train_reviews_df.comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-19T01:53:22.145062Z",
     "start_time": "2023-06-19T01:53:22.102429Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>comments</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Thank you very much Antonio ! All has been per...</td>\n",
       "      <td>thank much antonio all perfect stay appart per...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Very nice appartment in the old town of Lissab...</td>\n",
       "      <td>veri nice appart old town lissabon quit centra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>When travelling we're looking for kids friendl...</td>\n",
       "      <td>when travel we re look kid friend place stay a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>We've been in Lisbon in march 2013 (3 adults a...</td>\n",
       "      <td>we ve lisbon march 2013 3 adult 3 children the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Our host Antonio was very helpful with informa...</td>\n",
       "      <td>our host antonio help inform lissabon he pick ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                           comments  \\\n",
       "0      1  Thank you very much Antonio ! All has been per...   \n",
       "1      1  Very nice appartment in the old town of Lissab...   \n",
       "2      1  When travelling we're looking for kids friendl...   \n",
       "3      1  We've been in Lisbon in march 2013 (3 adults a...   \n",
       "4      1  Our host Antonio was very helpful with informa...   \n",
       "\n",
       "                                          clean_text  \n",
       "0  thank much antonio all perfect stay appart per...  \n",
       "1  veri nice appart old town lissabon quit centra...  \n",
       "2  when travel we re look kid friend place stay a...  \n",
       "3  we ve lisbon march 2013 3 adult 3 children the...  \n",
       "4  our host antonio help inform lissabon he pick ...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_reviews_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### VADER Clean Text Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-19T01:54:46.257654Z",
     "start_time": "2023-06-19T01:54:44.140097Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3505/2952864842.py:30: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.13 s, sys: 0 ns, total: 2.13 s\n",
      "Wall time: 2.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Vader requires a different preprocessing pipeline\n",
    "ct = CleanText()\n",
    "train_reviews_df['VADER_clean_text'] = ct.fit(train_reviews_df.comments).transform_vader(train_reviews_df.comments)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### BERT Clean Text Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-19T01:54:39.660538Z",
     "start_time": "2023-06-19T01:54:29.771972Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3505/2952864842.py:30: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.1 s, sys: 749 ms, total: 9.84 s\n",
      "Wall time: 9.86 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# BERT requires a different preprocessing pipeline\n",
    "ct = CleanText()\n",
    "train_reviews_df['BERT_clean_text'] = ct.fit(train_reviews_df.comments).transform_bert(train_reviews_df.comments)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Dense Embeddings Clean Text Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-19T01:54:08.278816Z",
     "start_time": "2023-06-19T01:54:00.276326Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3505/2952864842.py:30: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.47 s, sys: 529 ms, total: 8 s\n",
      "Wall time: 8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Dense Embeddings require a different preprocessing pipeline\n",
    "ct = CleanText()\n",
    "train_reviews_df['embedding_clean_text'] = ct.fit(train_reviews_df.comments).transform_embedding(\n",
    "    train_reviews_df.comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-19T01:54:51.655882Z",
     "start_time": "2023-06-19T01:54:51.460249Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Keep only those columns that will be used in from now on\n",
    "train_reviews_df = train_reviews_df[\n",
    "    ['index', 'clean_text', 'VADER_clean_text', 'BERT_clean_text', 'embedding_clean_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-19T01:54:52.402733Z",
     "start_time": "2023-06-19T01:54:52.384723Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>VADER_clean_text</th>\n",
       "      <th>BERT_clean_text</th>\n",
       "      <th>embedding_clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>thank much antonio all perfect stay appart per...</td>\n",
       "      <td>Thank you very much Antonio ! All has been per...</td>\n",
       "      <td>thank much antonio all perfect stay appartment...</td>\n",
       "      <td>thank much antonio all perfect stay appartment...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>veri nice appart old town lissabon quit centra...</td>\n",
       "      <td>Very nice appartment in the old town of Lissab...</td>\n",
       "      <td>very nice appartment old town lissabon quite c...</td>\n",
       "      <td>very nice appartment old town lissabon quite c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>when travel we re look kid friend place stay a...</td>\n",
       "      <td>When travelling we're looking for kids friendl...</td>\n",
       "      <td>when travelling we re looking kids friendly pl...</td>\n",
       "      <td>when travelling we re looking kids friendly pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>we ve lisbon march 2013 3 adult 3 children the...</td>\n",
       "      <td>We've been in Lisbon in march 2013 (3 adults a...</td>\n",
       "      <td>we ve lisbon march 2013 3 adults 3 children th...</td>\n",
       "      <td>we ve lisbon march 2013 3 adults 3 children th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>our host antonio help inform lissabon he pick ...</td>\n",
       "      <td>Our host Antonio was very helpful with informa...</td>\n",
       "      <td>our host antonio helpful information lissabon ...</td>\n",
       "      <td>our host antonio helpful information lissabon ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                         clean_text  \\\n",
       "0      1  thank much antonio all perfect stay appart per...   \n",
       "1      1  veri nice appart old town lissabon quit centra...   \n",
       "2      1  when travel we re look kid friend place stay a...   \n",
       "3      1  we ve lisbon march 2013 3 adult 3 children the...   \n",
       "4      1  our host antonio help inform lissabon he pick ...   \n",
       "\n",
       "                                    VADER_clean_text  \\\n",
       "0  Thank you very much Antonio ! All has been per...   \n",
       "1  Very nice appartment in the old town of Lissab...   \n",
       "2  When travelling we're looking for kids friendl...   \n",
       "3  We've been in Lisbon in march 2013 (3 adults a...   \n",
       "4  Our host Antonio was very helpful with informa...   \n",
       "\n",
       "                                     BERT_clean_text  \\\n",
       "0  thank much antonio all perfect stay appartment...   \n",
       "1  very nice appartment old town lissabon quite c...   \n",
       "2  when travelling we re looking kids friendly pl...   \n",
       "3  we ve lisbon march 2013 3 adults 3 children th...   \n",
       "4  our host antonio helpful information lissabon ...   \n",
       "\n",
       "                                embedding_clean_text  \n",
       "0  thank much antonio all perfect stay appartment...  \n",
       "1  very nice appartment old town lissabon quite c...  \n",
       "2  when travelling we re looking kids friendly pl...  \n",
       "3  we ve lisbon march 2013 3 adults 3 children th...  \n",
       "4  our host antonio helpful information lissabon ...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_reviews_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-19T01:55:55.800519Z",
     "start_time": "2023-06-19T01:55:46.419544Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save the cleaned dataframe\n",
    "train_reviews_df.to_csv('corpora/train_reviews_cleaned_df.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "@TODO do same pipelines on test data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Labeling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### **VADER based**\n",
    "\n",
    "VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media.\n",
    "\n",
    "VADER uses a combination of a sentiment lexicon (which is a list of lexical features e.g., words) which are generally labelled according to their semantic orientation as either positive or negative.\n",
    "\n",
    "VADER has been found to be quite successful when dealing with social media texts, NY Times editorials, movie reviews, and product reviews. This is because VADER not only tells about the Positivity and Negativity score but also tells us about how positive or negative a sentiment is.\n",
    "\n",
    "VADER analyses sentiments primarily based on certain key points:\n",
    "- **Punctuation**: The use of an exclamation mark(!), increases the magnitude of the intensity without modifying the semantic orientation. For example, “The food here is good!” is more intense than “The food here is good.” and an increase in the number of (!), increases the magnitude accordingly.\n",
    "- **Capitalization**: Using upper case letters to emphasize a sentiment-relevant word in the presence of other non-capitalized words, increases the magnitude of the sentiment intensity. For example, “The food here is GREAT!” conveys more intensity than “The food here is great!\n",
    "- **Degree modifiers**: Also called intensifiers, they impact the sentiment intensity by either increasing or decreasing the intensity. For example, “The service here is extremely good” is more intense than “The service here is good”, whereas “The service here is marginally good” reduces the intensity.\n",
    "- **Conjunctions**: Use of conjunctions like “but” signals a shift in sentiment polarity, with the sentiment of the text following the conjunction being dominant. “The food here is great, but the service is horrible” has mixed sentiment, with the latter half dictating the overall rating.\n",
    "- **Preceding Tri-gram**: By examining the tri-gram preceding a sentiment-laden lexical feature, we catch nearly 90% of cases where negation flips the polarity of the text. A negated sentence would be “The food here isn’t really all that great”.\n",
    "- **Emojis, Slangs, and Emoticons**: VADER performs very well with emojis, slangs, and acronyms in sentences.\n",
    "\n",
    "The Compound score is a metric that calculates the sum of all the lexicon ratings which have been normalized between -1(most extreme negative) and +1 (most extreme positive)\n",
    "\n",
    "-----\n",
    "\n",
    "**Strategy**\n",
    "\n",
    "Convert the compound scores to a normalized sentiment scale ranging from 1 (less positive) to 5 (more positive), with 2.5 representing a neutral sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-19T01:56:52.932264Z",
     "start_time": "2023-06-19T01:56:51.570493Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_reviews_cleaned_df = pd.read_csv('corpora/train_reviews_cleaned_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-19T01:56:56.734749Z",
     "start_time": "2023-06-19T01:56:56.714233Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def vader_sentiment_score(text):\n",
    "    score = analyzer.polarity_scores(text)['compound']\n",
    "    normalized_score = (score + 1) * 2.5 + 1\n",
    "    return normalized_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train_reviews_cleaned_df['VADER_sentiment'] = train_reviews_cleaned_df['VADER_clean_text'].apply(\n",
    "    lambda x: vader_sentiment_score(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "# Create a new column to store the VADER sentiment scores\n",
    "train_reviews_cleaned_df['VADER_sentiment'] = ''\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 100\n",
    "\n",
    "# Calculate the number of batches\n",
    "num_batches = math.ceil(len(train_reviews_cleaned_df) / batch_size)\n",
    "\n",
    "\n",
    "# Define the function for calculating VADER sentiment scores\n",
    "def calculate_vader_sentiment_batch(batch_df):\n",
    "    # Perform your VADER sentiment analysis here on the batch_df\n",
    "    # You can modify this function to fit your specific implementation\n",
    "    batch_df['VADER_sentiment'] = batch_df['VADER_clean_text'].apply(lambda x: vader_sentiment_score(x))\n",
    "    return batch_df\n",
    "\n",
    "\n",
    "# Iterate over the batches with tqdm for progress tracking\n",
    "for i in tqdm(range(num_batches)):\n",
    "    start_index = i * batch_size\n",
    "    end_index = (i + 1) * batch_size\n",
    "    batch_df = train_reviews_cleaned_df[start_index:end_index].copy()\n",
    "    batch_df = calculate_vader_sentiment_batch(batch_df)\n",
    "    train_reviews_cleaned_df[start_index:end_index] = batch_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_reviews_cleaned_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "positive_num = len(train_reviews_cleaned_df[train_reviews_cleaned_df['VADER_sentiment'] >= 2.5])\n",
    "negative_num = len(train_reviews_cleaned_df[train_reviews_cleaned_df['VADER_sentiment'] < 2.5])\n",
    "positive_num, negative_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_reviews_cleaned_df.to_csv('corpora/train_reviews_vader_sentiment_df.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### **BERT based**\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) is a Transformer-based machine learning technique for natural language processing (NLP) pre-training developed by Google."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Select 10 reviews for each index (hotel) as a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-19T01:57:23.626641Z",
     "start_time": "2023-06-19T01:57:22.405725Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_reviews_cleaned_df = pd.read_csv('corpora/train_reviews_cleaned_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-19T01:57:25.619856Z",
     "start_time": "2023-06-19T01:57:24.746335Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>BERT_clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>thank much antonio all perfect stay appartment...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>very nice appartment old town lissabon quite c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>when travelling we re looking kids friendly pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>we ve lisbon march 2013 3 adults 3 children th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>our host antonio helpful information lissabon ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>very nice place be large clean apartment x000d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>everything great antonio mother margarida good...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>a comfortable clean nice flat pleasant sunny t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>s jour id al nous avons t accueillis tr s chal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>i spent great time this place</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>this beautiful bright apartment residential ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>el departamento es tal cual aparece en las fot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>we d pleasant stay apartment expected service ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>flat clean well maintained nice touches master...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>a casa incr vel ideal para familia e ou grupo ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>appartement tr s agr able lumineux propre rien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>appartement sympa h te disponible proche bus e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>we really nice stay everything expected with b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>due late check out cleaning ladies not time pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "      <td>a great place stay</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                    BERT_clean_text\n",
       "0      1  thank much antonio all perfect stay appartment...\n",
       "1      1  very nice appartment old town lissabon quite c...\n",
       "2      1  when travelling we re looking kids friendly pl...\n",
       "3      1  we ve lisbon march 2013 3 adults 3 children th...\n",
       "4      1  our host antonio helpful information lissabon ...\n",
       "5      1  very nice place be large clean apartment x000d...\n",
       "6      1  everything great antonio mother margarida good...\n",
       "7      1  a comfortable clean nice flat pleasant sunny t...\n",
       "8      1  s jour id al nous avons t accueillis tr s chal...\n",
       "9      1                      i spent great time this place\n",
       "10     2  this beautiful bright apartment residential ar...\n",
       "11     2  el departamento es tal cual aparece en las fot...\n",
       "12     2  we d pleasant stay apartment expected service ...\n",
       "13     2  flat clean well maintained nice touches master...\n",
       "14     2  a casa incr vel ideal para familia e ou grupo ...\n",
       "15     2  appartement tr s agr able lumineux propre rien...\n",
       "16     2  appartement sympa h te disponible proche bus e...\n",
       "17     2  we really nice stay everything expected with b...\n",
       "18     2  due late check out cleaning ladies not time pr...\n",
       "19     2                                 a great place stay"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an empty DataFrame to store the selected comments\n",
    "selected_df = pd.DataFrame(columns=['index', 'BERT_clean_text'])\n",
    "\n",
    "# Iterate over each unique index\n",
    "for index in train_reviews_cleaned_df['index'].unique():\n",
    "    # Filter the DataFrame to get comments for the current index\n",
    "    comments = train_reviews_cleaned_df[train_reviews_cleaned_df['index'] == index].head(10)\n",
    "\n",
    "    # Select only the 'index' and 'BERT_clean_text' columns\n",
    "    selected_comments = comments[['index', 'BERT_clean_text']]\n",
    "\n",
    "    # Concatenate the selected comments to the new DataFrame\n",
    "    selected_df = pd.concat([selected_df, selected_comments])\n",
    "\n",
    "# Reset the index of the new DataFrame\n",
    "selected_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "selected_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-19T01:58:00.947594Z",
     "start_time": "2023-06-19T01:57:51.176115Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save the cleaned dataframe\n",
    "train_reviews_cleaned_df.to_csv('corpora/train_reviews_sentiment_df.csv', index=False)\n",
    "selected_df.to_csv('corpora/samples_train_reviews_sentiment_df.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Sentiment Analysis with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "samples_train_reviews_sentiment = pd.read_csv('corpora/samples_train_reviews_sentiment_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bert_score(review):\n",
    "    if not review:\n",
    "        return 2  # Set score to 1 if the review string is empty\n",
    "\n",
    "    tokens = bert_base_multilingual_tokenizer.encode_plus(\n",
    "        review,\n",
    "        return_tensors='pt',\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512  # Set the maximum length as per your requirement\n",
    "    )\n",
    "    input_ids = tokens['input_ids']\n",
    "    attention_mask = tokens['attention_mask']\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_base_multilingual_model(input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits\n",
    "    predicted_label = torch.argmax(logits, dim=1) + 1\n",
    "\n",
    "    return predicted_label.item()\n",
    "\n",
    "\n",
    "def sentiment_score(reviews):\n",
    "    predicted_labels = []\n",
    "    for review in tqdm(reviews, desc=\"Processing reviews\"):\n",
    "        if not isinstance(review, str) or not review.strip():\n",
    "            predicted_labels.append(2)  # Set score to 1 if the review is empty or not a string\n",
    "        else:\n",
    "            score = bert_score(review)\n",
    "            predicted_labels.append(score)\n",
    "\n",
    "    return predicted_labels\n",
    "\n",
    "\n",
    "# Process sentiment scores for the reviews\n",
    "sentiment_scores = sentiment_score(samples_train_reviews_sentiment['BERT_clean_text'])\n",
    "\n",
    "# Assign sentiment scores to the DataFrame\n",
    "samples_train_reviews_sentiment['BERT_sentiment'] = np.nan\n",
    "samples_train_reviews_sentiment.loc[:len(sentiment_scores) - 1, 'BERT_sentiment'] = sentiment_scores\n",
    "\n",
    "samples_train_reviews_sentiment.to_csv(\"samples_train_reviews_sentiment_final.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### **Comparison of Sentiment Labels**\n",
    "The investigation aimed to determine if a high percentage of different \"sentiment\" labels were associated with the two methods.\n",
    "\n",
    "If the two methods yielded almost identical labels, a single dataset would be used.\n",
    "\n",
    "----------------------------\n",
    "**Results**\n",
    "\n",
    "An analysis was conducted on selected reviews that received different labels through the two methodologies (Bert-based and VADER-based).\n",
    "\n",
    "**Insights**\n",
    "\n",
    "Through a comprehensive exploration of the dataset, it was observed that the sentiment labels assigned by VADER were accurate for English text. Reviews containing words like \"good\" or \"comfortable\" obtained a label of \"1\". However, VADER's accuracy was less reliable when applied to non-English text, as it relies on a pre-trained lexicon with sentiment scores specifically for English words. Consequently, its performance may not be as accurate or reliable when applied to languages other than English.\n",
    "\n",
    "As a result, the decision was made to use BERT, a multilingual model, for labeling the reviews. BERT produced more accurate results compared to VADER.\n",
    "\n",
    "Additionally, it is worth noting that this experiment provided an opportunity to explore an alternative technique and discover the existence and application of VADER-Sentiment Analyzer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
